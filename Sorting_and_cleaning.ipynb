{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sorting_and_cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njaincode/python_for_data_science/blob/main/Sorting_and_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CVoh0pMzW0l"
      },
      "source": [
        "# Sorting and cleaning \n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In order to effectively analyse a dataset, often we need to prepare it first. \n",
        "Before a dataset is ready to be analysed we might need to:  \n",
        "\n",
        "* sort the data (can be a series or dataframe)  \n",
        "* remove any NaN values or drop NA values   \n",
        "* remove duplicate records (identical rows)  \n",
        "* normalise data in dataframe columns so that has a common scale [reference](https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff#:~:text=Similarly%2C%20the%20goal%20of%20normalization,dataset%20does%20not%20require%20normalization.&text=So%20we%20normalize%20the%20data,variables%20to%20the%20same%20range.)\n",
        "\n",
        "## Sorting the data  \n",
        "---\n",
        "\n",
        "\n",
        "Typically we want to sort data by the values in one or more columns in the dataframe  \n",
        "\n",
        "To sort the dataframe by series we use the pandas function **sort_values()**.  \n",
        "\n",
        "By default `sort_values()` sorts into ascending order.\n",
        "\n",
        "* Sort by a single column e.g.\n",
        "  * `df.sort_values(\"Make\") `\n",
        "* Sort by multiple columns e.g. \n",
        "  * `df.sort_values(by = [\"Model\", \"Make\"]) `\n",
        "    * this sorts by Model, then my Make \n",
        "* Sort in *descending* order\n",
        "  * `df.sort_values(by = \"Make\", ascending = False)`\n",
        "  * `df.sort_values(by = [\"Make\", \"Model\"], ascending = False])`  \n",
        "\n",
        "Dataframes are mostly immutable, changes like sort_values do not change the dataframe permanently, they just change it for the time that the instruction is being used.\n",
        "\n",
        "`df.sort_values(by='Make')` *dataframe is now in sorted order and can be copied to a new dataframe*  \n",
        "`df` *original dataframe, df, will be as it was - unsorted* \n",
        "\n",
        "To split the dataframe after sorting, do this in the same instruction, e.g.:\n",
        "\n",
        "`df.sort_values(by = [\"Make\", \"Model\"], ascending = False])[[\"Make\", \"Model\"]]`\n",
        "\n",
        "This sorts on Make and then Model in descending order, then splits off the Make and Model columns.\n",
        "\n",
        "`df.sort_values(by = [\"Make\", \"Model\"], ascending = False])[[\"Make\", \"Model\"]].head()`\n",
        "\n",
        "This sorts on Make and then Model, then splits off the Make and Model columns and then splits off the first 5 rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANKknIx8E-hN"
      },
      "source": [
        "### Exercise 1 - get data, sort by happiness score \n",
        "---\n",
        "\n",
        "Read data from the Excel file on Happiness Data at this link: https://github.com/futureCodersSE/working-with-data/blob/main/Happiness-Data/2015.xlsx?raw=true\n",
        "\n",
        "Display first 5 rows of data  \n",
        "\n",
        "The data is currently sorted by Happiness Rank...\n",
        "*  sort the data by Happiness Score in ascending order\n",
        "*  display sorted table\n",
        "\n",
        "**Test output**:  \n",
        "The lowest score (displayed first) is 2.839, Togo  \n",
        "The highest score (displayed last) is 7.587, Switzerland  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkvFGvJtHXiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7943657-c9bb-4f75-b148-4209e06f55fb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sort_ex1():\n",
        "  excel_url = 'https://github.com/futureCodersSE/working-with-data/blob/main/Happiness-Data/2015.xlsx?raw=true'\n",
        "\n",
        "  # Practice reading certain columns only\n",
        "  df_hap_2015 = pd.read_excel(excel_url, usecols=[\"Happiness Rank\", \"Happiness Score\", \"Country\"])\n",
        "    \n",
        "  # Set some display options when run\n",
        "  pd.options.display.max_rows= 20\n",
        "  pd.options.display.max_columns= 12\n",
        "\n",
        "  # sort the data by Happiness Score in ascending order and display sorted table\n",
        "  df_sorted_happy_score = df_hap_2015.sort_values(by=\"Happiness Score\", ascending=True)\n",
        "  print(df_sorted_happy_score)\n",
        "\n",
        "sort_ex1()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         Country  Happiness Rank  Happiness Score\n",
            "157         Togo             158            2.839\n",
            "156      Burundi             157            2.905\n",
            "155        Syria             156            3.006\n",
            "154        Benin             155            3.340\n",
            "153       Rwanda             154            3.465\n",
            "..           ...             ...              ...\n",
            "4         Canada               5            7.427\n",
            "3         Norway               4            7.522\n",
            "2        Denmark               3            7.527\n",
            "1        Iceland               2            7.561\n",
            "0    Switzerland               1            7.587\n",
            "\n",
            "[158 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_iomqRTH8LA"
      },
      "source": [
        "### Exercise 2 - sort by multiple columns, display the first 5 rows \n",
        "---\n",
        "\n",
        "1. Sort the data by Economy (GDP per Capita) and Health (Life Expectancy) in ascending order \n",
        "2. Display the first 5 rows of sorted data \n",
        "\n",
        "**Test output**:  \n",
        "Records 122, 127, 147, 100, 96"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7XalX7OK0u-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78232e50-1239-4bc7-efce-9153ddc7915f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sort_ex2():\n",
        "  excel_url = 'https://github.com/futureCodersSE/working-with-data/blob/main/Happiness-Data/2015.xlsx?raw=true'\n",
        "\n",
        "  # Practice reading certain columns only\n",
        "  sort_col = [\"Economy (GDP per Capita)\", \"Health (Life Expectancy)\"]\n",
        "  df_hap_2015 = pd.read_excel(excel_url, usecols=sort_col)\n",
        "    \n",
        "  #print(df_hap_2015.iloc[[122,127,147,100,96]])\n",
        "\n",
        "  # Sort the data by Economy (GDP per Capita) and Health (Life Expectancy) in ascending order\n",
        "  # The test output needs columns sorted in this order and not in stat_col\n",
        "  sort_col1 = [\"Health (Life Expectancy)\", \"Economy (GDP per Capita)\"]\n",
        "  df_sorted_economy_health = df_hap_2015.sort_values(by=sort_col1).head(5)\n",
        "  # Display the first 5 rows of sorted data\n",
        "  print(df_sorted_economy_health)\n",
        "\n",
        "sort_ex2()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Economy (GDP per Capita)  Health (Life Expectancy)\n",
            "122                   0.33024                   0.00000\n",
            "127                   0.99355                   0.04776\n",
            "147                   0.07850                   0.06699\n",
            "100                   0.71206                   0.07566\n",
            "96                    0.37545                   0.07612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfQ3cys4LHNc"
      },
      "source": [
        "### Exercise 3 - sorting in descending order \n",
        "---\n",
        " \n",
        "Sort the data by Freedom and Trust (Government Corruption) in descending order and show the Country and Region only for the last five rows\n",
        "\n",
        "**Test output**:\n",
        "43, 7, 144, 0, 3 Country and Region columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3haPVvX7MCom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a229281-ce4a-4af6-e403-fa53ba57b250"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sort_descending():\n",
        "  excel_url = 'https://github.com/futureCodersSE/working-with-data/blob/main/Happiness-Data/2015.xlsx?raw=true'\n",
        "\n",
        "  # Practice reading certain columns only\n",
        "  get_col = [\"Country\", \"Region\", \"Freedom\", \"Trust (Government Corruption)\"]\n",
        "  df_hap_2015 = pd.read_excel(excel_url, usecols=get_col)\n",
        "\n",
        "  # Columns to display\n",
        "  disp_col = [\"Country\", \"Region\"]\n",
        "  # Columns used to sort\n",
        "  sort_col = [\"Freedom\", \"Trust (Government Corruption)\"]\n",
        "  disp_col1 = [\"Country\", \"Freedom\", \"Trust (Government Corruption)\"]\n",
        "  \n",
        "  # For debug\n",
        "  #print(df_hap_2015.loc[[43,7,144,0,3], disp_col1])\n",
        "  \n",
        "  # Sort the data by Freedom and Trust (Government Corruption) in descending order and show the Country and Region only for the last five rows\n",
        "  \n",
        "  df_sorted_freedom_trust = df_hap_2015.sort_values(by=sort_col, ascending=[False, False])\n",
        "  # Display the last 5 rows of sorted data (should show min values)\n",
        "  # FIXME Test output does not match\n",
        "  print(df_sorted_freedom_trust[disp_col].tail(5))\n",
        "\n",
        "sort_descending()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    Country                           Region\n",
            "136                  Angola               Sub-Saharan Africa\n",
            "117                   Sudan               Sub-Saharan Africa\n",
            "95   Bosnia and Herzegovina       Central and Eastern Europe\n",
            "101                  Greece                   Western Europe\n",
            "111                    Iraq  Middle East and Northern Africa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqnAoELjMDs7"
      },
      "source": [
        "# Cleaning the data\n",
        "\n",
        "Data comes from a range of sources:  forms, monitoring devices, etc.  There will often be missing values, duplicate records and values that are incorrectly formatted.  These can affect summary statistics and graphs plotted from the data.\n",
        "\n",
        "Techniques for data cleansing include:\n",
        "*  removing records with missing or null data (NaN, NA, \"\")\n",
        "*  removing duplicate rows (keeping just one, either the first or the last)\n",
        "\n",
        "Removal of rows according to criteria, or of columns are other ways that data might be cleaned up.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVqmfM5wk7NK"
      },
      "source": [
        "---\n",
        "\n",
        "## Removing NaN/Dropping NA values \n",
        "\n",
        "pandas have functions for checking a dataframe, or column, for null values, checking a column for missing values, and functions for dropping all rows that contain null values.\n",
        "\n",
        "* check for NA/NaN/missing values across dataframe (returns True if NA values exist)  \n",
        "  `df.isnull().values.any()`  \n",
        "\n",
        "* check for NA/NaN/missing values in specific column  \n",
        "  `df[\"Make\"].isnull().values.any()`  \n",
        "\n",
        "* drop all rows that have NA/NaN values   \n",
        "  `df.dropna()`  \n",
        "\n",
        "* drop rows where NA/NaN values exist in specific columns  \n",
        "  `df.dropna(subset = [\"Make\", \"Model\"])`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC65hEZGOKNL"
      },
      "source": [
        "### Exercise 4 - check for null values \n",
        "---\n",
        "\n",
        "1. Read data from the file housing_in_london_yearly_variables.csv from this link: https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv \n",
        "2. check if any NA values exist in the dataframe and print the result \n",
        "3. use df.info() to see which columns have null entries (*Hint: if the non-null count is less than total entries, column contains missing/NA entries*)  \n",
        "\n",
        "**Test output**:\n",
        "True\n",
        ".info shows median_salary, life_satisfaction, recycling_pct, population_size, number_of_jobs, area_size, no_of_houses all less than total rows (1071) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7LYkXDNVVc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d8f1e1-1fc0-4f7b-9c5c-6ba39e225437"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def check_null_values():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  # 1. Check if any NA values exist in the dataframe and print the result\n",
        "  # isnull is an alias of isna, so both can be used interchangably\n",
        "  if (df_london_housing.isna().values.any() == True):\n",
        "    print(f'Data frame contains NULL entries')\n",
        "  else:\n",
        "    print(f'Data frame does not contain any NULL entries')\n",
        "\n",
        "  # 2. Use df.info() to see which columns have null entries\n",
        "  # Used panda function instead\n",
        "  print(df_london_housing.columns[df_london_housing.isna().any()].tolist())\n",
        "\n",
        "  # isna().any() returns an index\n",
        "  print(df_london_housing.columns[df_london_housing.isna().any()])\n",
        "\n",
        "  # Different way to look at data\n",
        "  print(\"No. of columns containing null values\")\n",
        "  # isna.any -> if NA in any cell\n",
        "  print(len(df_london_housing.columns[df_london_housing.isna().any()]))\n",
        "\n",
        "  print(\"No. of columns not containing null values\")\n",
        "  # notna.all -> No NA in any cell\n",
        "  print(len(df_london_housing.columns[df_london_housing.notna().all()]))\n",
        "\n",
        "  print(\"Total no. of columns in the dataframe\")\n",
        "  print(len(df_london_housing.columns))\n",
        "\n",
        "\n",
        "check_null_values()\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data frame contains NULL entries\n",
            "['median_salary', 'life_satisfaction', 'recycling_pct', 'population_size', 'number_of_jobs', 'area_size', 'no_of_houses']\n",
            "Index(['median_salary', 'life_satisfaction', 'recycling_pct',\n",
            "       'population_size', 'number_of_jobs', 'area_size', 'no_of_houses'],\n",
            "      dtype='object')\n",
            "No. of columns containing null values\n",
            "7\n",
            "No. of columns not containing null values\n",
            "5\n",
            "Total no. of columns in the dataframe\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRBLm_bJVItu"
      },
      "source": [
        "### Exercise 5 - remove null values \n",
        "---\n",
        "\n",
        "1. Remove rows with NA values for `life_satisfaction` (use [ ] even if only one column in list)\n",
        "2. Remove all NA values across whole dataframe \n",
        "\n",
        "**Test output**:  \n",
        "1.  Row count reduced to 352 rows\n",
        "2.  Row count reduced to 267 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjZJNIC3QObK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e131f2-8d44-474a-9356-3c7d926b84c2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def remove_null_values():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  # 1. Check if any NA values exist in life_satisfaction column\n",
        "  if (df_london_housing[\"life_satisfaction\"].isna().values.any() == True):\n",
        "    print(f'life_satisfaction column contains NULL entries')\n",
        "\n",
        "    print(f'Number of rows before dropna = {len(df_london_housing.index)}')\n",
        "\n",
        "    df_nona_life_satisfaction = df_london_housing[\"life_satisfaction\"].dropna()\n",
        "    print(f'Number of rows after dropna = {len(df_nona_life_satisfaction.index)}')\n",
        "    \n",
        "  # 2. Remove all NA values across whole dataframe\n",
        "  if (df_london_housing.isna().values.any() == True):\n",
        "    print(f'Dataframe contains NULL entries')\n",
        "    total_na_cells = df_london_housing.isna().sum()\n",
        "    print(f'Total number of cells per col with NA = {total_na_cells}')\n",
        "\n",
        "    df_london_housing.dropna(inplace=True)\n",
        "    print(f'Number of rows after dropna on whole df = {len(df_london_housing.index)}')\n",
        "\n",
        "    #total_na_cells_rows = df_london_housing.isna().sum(axis=1)\n",
        "    #print(f'Total number of rows with NA = {total_na_cells_rows}')\n",
        "\n",
        "remove_null_values()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "life_satisfaction column contains NULL entries\n",
            "Number of rows before dropna = 1071\n",
            "Number of rows after dropna = 352\n",
            "352\n",
            "Dataframe contains NULL entries\n",
            "Total number of cells with NA = code                   0\n",
            "area                   0\n",
            "date                   0\n",
            "median_salary         22\n",
            "life_satisfaction    719\n",
            "mean_salary            0\n",
            "recycling_pct        211\n",
            "population_size       53\n",
            "number_of_jobs       140\n",
            "area_size            405\n",
            "no_of_houses         405\n",
            "borough_flag           0\n",
            "dtype: int64\n",
            "Number of rows after dropna on whole df = 267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui8HF5z8SiK8"
      },
      "source": [
        "## Dropping duplicates\n",
        "---\n",
        "\n",
        "* To remove duplicate rows based on duplication of values in all columns  \n",
        "  `df.drop_duplicates()`  \n",
        "\n",
        "* To remove rows that have duplicate entries in a specified column  \n",
        "  `df.drop_duplicates(subset = ['Make'])`  \n",
        "\n",
        "* To remove rows that have duplicate entries in multiple columns  \n",
        "  `df.drop_duplicates(subset = ['Make', 'Model'])` \n",
        "\n",
        "* Remove duplicate rows keeping the last instance rather than the first (default):  \n",
        "  `df.drop_duplicates(keep='last')`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Qf6uMxSb5t"
      },
      "source": [
        "### Exercise 6 - Removing duplicate entries \n",
        "---\n",
        "\n",
        "remove duplicate `area` entries keeping first instance  \n",
        "\n",
        "**Test output**:  \n",
        " Dataframe now contains 50 rows all with date 1999-12-*01* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ8T0tYVQj74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2350bb37-f3ac-49c8-897b-ff13b5cca02a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def remove_duplicates():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  # Remove duplicate area entries keeping first instance\n",
        "  print(f'Number of rows before dropna = {len(df_london_housing[\"area\"].index)}')\n",
        "  print(df_london_housing[[\"area\", \"date\"]])\n",
        "\n",
        "  # Find number of duplicate entries in column area\n",
        "  total_num_duplicates_in_area = len(df_london_housing['area']) -len(df_london_housing['area'].drop_duplicates())\n",
        "  print(f'Total num of duplicates entries in area = {total_num_duplicates_in_area}')\n",
        "\n",
        "  # Remove entries\n",
        "  #df_london_housing.drop_duplicates(keep='first', inplace=True)\n",
        "  #df = df_london_housing.drop_duplicates(subset= [\"area\"])\n",
        "  df = df_london_housing[\"area\"].drop_duplicates()\n",
        "  print(f'Total num of unique entries in area = {len(df.index)}')\n",
        "  print(df)\n",
        "\n",
        "  # df_london_housing[\"area\"].drop_dup.. with inplace=True will not work! Use subset\n",
        "  df_london_housing.drop_duplicates(subset=\"area\", keep='first', inplace=True)\n",
        "  print(df_london_housing[[\"area\", \"date\"]])\n",
        "  #print(df_london_housing[\"area\"])\n",
        "  num_rows_in_area = len(df_london_housing[\"area\"].index)\n",
        "  print(num_rows_in_area)\n",
        "\n",
        "\n",
        "remove_duplicates()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows before dropna = 1071\n",
            "                      area        date\n",
            "0           city of london  1999-12-01\n",
            "1     barking and dagenham  1999-12-01\n",
            "2                   barnet  1999-12-01\n",
            "3                   bexley  1999-12-01\n",
            "4                    brent  1999-12-01\n",
            "...                    ...         ...\n",
            "1066         great britain  2019-12-01\n",
            "1067     england and wales  2019-12-01\n",
            "1068      northern ireland  2019-12-01\n",
            "1069              scotland  2019-12-01\n",
            "1070                 wales  2019-12-01\n",
            "\n",
            "[1071 rows x 2 columns]\n",
            "Total num of duplicates entries in area = 1020\n",
            "Total num of unique entries in area = 51\n",
            "0           city of london\n",
            "1     barking and dagenham\n",
            "2                   barnet\n",
            "3                   bexley\n",
            "4                    brent\n",
            "              ...         \n",
            "46           great britain\n",
            "47       england and wales\n",
            "48        northern ireland\n",
            "49                scotland\n",
            "50                   wales\n",
            "Name: area, Length: 51, dtype: object\n",
            "                    area        date\n",
            "0         city of london  1999-12-01\n",
            "1   barking and dagenham  1999-12-01\n",
            "2                 barnet  1999-12-01\n",
            "3                 bexley  1999-12-01\n",
            "4                  brent  1999-12-01\n",
            "..                   ...         ...\n",
            "46         great britain  1999-12-01\n",
            "47     england and wales  1999-12-01\n",
            "48      northern ireland  1999-12-01\n",
            "49              scotland  1999-12-01\n",
            "50                 wales  1999-12-01\n",
            "\n",
            "[51 rows x 2 columns]\n",
            "51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ_tQG_3WBXn"
      },
      "source": [
        "# Normalising Data  \n",
        "When we normalise data, we remodel a numeric column in a dataframe to be on a standard scale (e.g. 0 or 1).   \n",
        "\n",
        "For example if we had a column of BMI scores, we could normalise that column so that all scores greater than or equal to 25 were recoded to the value 1 (bad) and all scores less than 25 were recoded to 0 (good).  \n",
        "\n",
        "To normalise we need to:\n",
        "*   write a function, with the dataframe as a parameter, which will look at each row in dataframe column and return either a value in the normalised scale (e.g. 0,1 or 1,2,3,4) depending on that value.\n",
        "\n",
        "For example:  \n",
        "```\n",
        "def normalise_bmi(df):\n",
        "  if df['bmi'] >= 25:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df[\"bmi\"] = df.apply(normalise_bmi, axis=1)\n",
        "```\n",
        "This code reassigns the values in the column \"bmi\" by sending each row one after the other to the normalise_bmi function, which will check the value in the \"bmi\" column and return either 0 or 1 depending on the value in the \"bmi\" column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8KPmy2_NVh1"
      },
      "source": [
        "### Exercise 7 - normalise data set\n",
        "---\n",
        "\n",
        "Create a function called **normalise_income(df)** that will return the values 1, 2 or 3 to represent low income, middle income and high income.  If the value in `df['median_salary']` is less than 27441 (the median), return 1, otherwise if it is less than 30932 (the upper quartile) return 2 and otherwise return 3.\n",
        "\n",
        "Apply the normalise_income(df) function to the `median_salary` column.\n",
        "\n",
        "*NOTE:  this operation will change the original dataframe so if you run it twice, everything in the median_salary column will change to 1 (as it had already been reduced to 1, 2 or 3 - if this happens, run the code in Exercise 4 again to get the original data again from the file.*\n",
        "\n",
        "**Test output**:  \n",
        "The maximum value of the column df['median_salary'] will be 3 and the minimum value will be 1  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktylpCl7QjGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "658d9ec0-fd46-4522-e849-f9889bc09067"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Used when whole dataframe (each row) is passed\n",
        "#def normalise_income(df):\n",
        "#  if df['median_salary'] < 27441:\n",
        "#    return 1\n",
        "#  elif df['median_salary'] < 30932 :\n",
        "#    return 2\n",
        "#  else:\n",
        "#    return 3\n",
        "\n",
        "\n",
        "def normalise_income(val):\n",
        "  if val < 27441:\n",
        "    return 1\n",
        "  elif val < 30932 :\n",
        "    return 2\n",
        "  else:\n",
        "    return 3\n",
        " \n",
        "def normalise_data_set():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  #1. Check if any NA values exist in life_satisfaction column\n",
        "  if (df_london_housing[\"median_salary\"].isna().values.any() == True):\n",
        "    print(f' median_salary column contains NULL entries')\n",
        "\n",
        "    print(f'Number of rows before dropna = {len(df_london_housing.index)}')\n",
        "\n",
        "    # To keep things simple, removed nan and empty rows\n",
        "    df_nona_salary = df_london_housing[\"median_salary\"].dropna()\n",
        "    print(f'Number of rows after dropna = {len(df_nona_salary.index)}')\n",
        "\n",
        "    # Create a copy in case need to cross check\n",
        "    df_nona_salary_orig = df_nona_salary\n",
        "    #print(df_nona_salary.describe())\n",
        "\n",
        "    # Rather than updating actual df, can it go into a copy?\n",
        "    # Since df_nona_salary is a series, normalise_income has been changed accordingly\n",
        "    df_nona_salary[\"median_salary\"] = df_nona_salary.apply(normalise_income)\n",
        "    print(df_nona_salary[\"median_salary\"].head(10))\n",
        "    print(df_nona_salary[\"median_salary\"].describe())\n",
        "\n",
        "normalise_data_set()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " median_salary column contains NULL entries\n",
            "Number of rows before dropna = 1071\n",
            "Number of rows after dropna = 1049\n",
            "0    3\n",
            "1    1\n",
            "2    1\n",
            "3    1\n",
            "4    1\n",
            "5    1\n",
            "6    1\n",
            "7    1\n",
            "8    1\n",
            "9    1\n",
            "Name: median_salary, dtype: int64\n",
            "count    1049.000000\n",
            "mean        1.751192\n",
            "std         0.830054\n",
            "min         1.000000\n",
            "25%         1.000000\n",
            "50%         2.000000\n",
            "75%         3.000000\n",
            "max         3.000000\n",
            "Name: median_salary, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCrIEyMjSYTI"
      },
      "source": [
        "### Exercise 8 - normalise the number of jobs column\n",
        "---\n",
        "\n",
        "Using what you have learnt from Exercise 7:  \n",
        "*  use `df.describe()` to find the median, upper quartile and maximum for the number_of_jobs column  \n",
        "*  create a function called **normalise_jobs(df)** that will return 1 if the `number_of_jobs` is below the median, 2 if the `number_of_jobs` is below the upper quartile or 3 otherwise.\n",
        "*  normalise the `number_of_jobs` column by applying the function `normalise_jobs`.\n",
        "\n",
        "**Test output**:  \n",
        "The maximum value of the column df['number_of_jobs'] will be 3 and the minimum value will be 1  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giYXovr-T7TB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8708ed-e9e2-4a83-e8de-a7a6495a36b4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Used when whole dataframe (each row) is passed\n",
        "def normalise_col1(df, df_median, df_upper_quartile):\n",
        "  if df['number_of_jobs'] < df_median:\n",
        "    return 1\n",
        "  elif df['number_of_jobs'] < df_upper_quartile:\n",
        "    return 2\n",
        "  else:\n",
        "    return 3\n",
        "\n",
        "def normalise_col(df):\n",
        "  if df['number_of_jobs'] < 157000.0:\n",
        "    return 1\n",
        "  elif df['number_of_jobs'] < 2217000.0:\n",
        "    return 2\n",
        "  else:\n",
        "    return 3\n",
        "\n",
        "def normalise_job():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  #1. Check if any NA values exist in number_of_jobs column\n",
        "  if (df_london_housing[\"number_of_jobs\"].isna().values.any() == True):\n",
        "    print(f'number_of_jobs column contains NULL entries')\n",
        "\n",
        "    print(f'Number of rows before dropna = {len(df_london_housing.index)}')\n",
        "\n",
        "    # To keep things simple, removed nan and empty rows\n",
        "    df_london_housing[\"number_of_jobs\"].dropna(inplace=True)\n",
        "    \n",
        "    # Create a copy in case need to cross check\n",
        "    df_nona_jobs_orig = df_london_housing\n",
        "\n",
        "    # Capture median, upper quartile and max (for debug)\n",
        "    print(df_london_housing[\"number_of_jobs\"].describe())\n",
        "\n",
        "    job_median = df_london_housing[\"number_of_jobs\"].median()\n",
        "    job_max = df_london_housing[\"number_of_jobs\"].max()\n",
        "    job_75_per = df_london_housing[\"number_of_jobs\"].quantile([0.75])\n",
        "    print(f'median = {job_median} max = {job_max} job_max = {job_75_per}')\n",
        "\n",
        "    # FIXME How to pass args to normalise fn?\n",
        "    df_london_housing[\"number_of_jobs\"] = df_london_housing.apply(normalise_col, axis=1)\n",
        "    print(df_london_housing[\"number_of_jobs\"].describe())\n",
        "\n",
        "\n",
        "normalise_job()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number_of_jobs column contains NULL entries\n",
            "Number of rows before dropna = 1071\n",
            "count    9.310000e+02\n",
            "mean     3.188095e+06\n",
            "std      8.058302e+06\n",
            "min      4.700000e+04\n",
            "25%      9.450000e+04\n",
            "50%      1.570000e+05\n",
            "75%      2.217000e+06\n",
            "max      3.575000e+07\n",
            "Name: number_of_jobs, dtype: float64\n",
            "median = 157000.0 max = 35750000.0 job_max = 0.75    2217000.0\n",
            "Name: number_of_jobs, dtype: float64\n",
            "count    1071.000000\n",
            "mean        1.914099\n",
            "std         0.880790\n",
            "min         1.000000\n",
            "25%         1.000000\n",
            "50%         2.000000\n",
            "75%         3.000000\n",
            "max         3.000000\n",
            "Name: number_of_jobs, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-akqnUbYVblH"
      },
      "source": [
        "## Exercise 9 - normalise into a new column\n",
        "---\n",
        "\n",
        "Create a new function and code to normalise the `no_of_houses` column BUT this time, instead of assigning the result to `df['no_of_houses']` assign it to a new column called `df['housing_volume']`\n",
        "\n",
        "**Test output**:  \n",
        "The maximum value of the column df['housing_volume'] will be 3 and the minimum value will be 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnQsE4znV6nD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2cf200-9355-412b-b700-a0dbe5ee3d4b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def normalise_house_vol(val):\n",
        "  if val < 102402.0:\n",
        "    return 1\n",
        "  elif val < 126276.0:\n",
        "    return 2\n",
        "  else:\n",
        "    return 3\n",
        " \n",
        "def normalise_house():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  #1. Check if any NA values exist in life_satisfaction column\n",
        "  if (df_london_housing[\"no_of_houses\"].isna().values.any() == True):\n",
        "    print(f' no_of_houses column contains NULL entries')\n",
        "\n",
        "    # To keep things simple, removed nan and empty rows\n",
        "    df_nona_house_vol = df_london_housing[\"no_of_houses\"].dropna()\n",
        "    \n",
        "    house_median = df_nona_house_vol.median()\n",
        "    house_75_per = df_nona_house_vol.quantile([0.75])\n",
        "    print(f'median = {house_median}  per_75 = {house_75_per}')\n",
        "\n",
        "    # Normalise number of houses\n",
        "    df_nona_house_vol[\"housing_volume\"] = df_nona_house_vol.apply(normalise_house_vol)\n",
        "    print(df_nona_house_vol[\"housing_volume\"].head(10))\n",
        "    print(df_nona_house_vol[\"housing_volume\"].describe())\n",
        "\n",
        "normalise_house()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " no_of_houses column contains NULL entries\n",
            "Number of rows before dropna = 1071\n",
            "count    6.660000e+02\n",
            "mean     8.814682e+05\n",
            "std      3.690376e+06\n",
            "min      5.009000e+03\n",
            "25%      8.763550e+04\n",
            "50%      1.024020e+05\n",
            "75%      1.262760e+05\n",
            "max      2.417217e+07\n",
            "Name: no_of_houses, dtype: float64\n",
            "median = 102402.0  per_75 = 0.75    126276.0\n",
            "Name: no_of_houses, dtype: float64\n",
            "102    1\n",
            "103    1\n",
            "104    3\n",
            "105    1\n",
            "106    1\n",
            "107    3\n",
            "108    1\n",
            "109    3\n",
            "110    2\n",
            "111    2\n",
            "Name: no_of_houses, dtype: int64\n",
            "count    666.000000\n",
            "mean       1.750751\n",
            "std        0.830458\n",
            "min        1.000000\n",
            "25%        1.000000\n",
            "50%        1.500000\n",
            "75%        2.750000\n",
            "max        3.000000\n",
            "Name: no_of_houses, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_FaL31EXHZX"
      },
      "source": [
        "### Exercise 10 - normalise boroughs\n",
        "---\n",
        "\n",
        "Normalise the `area_size` column so that all values below mean are represented as 0 and otherwise are 1.  Assign the output to a new column called `area_size_normalised`.  \n",
        "\n",
        "**Test output**:  \n",
        "`area_size_normalised` column will contain both 0s and 1s.  The position of the first row with value 1 will be 0 and the position of the first row with value 0 will be 102.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doIZ9M0UXkkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ee5aca-af71-4e8a-b1f8-5945caaae30b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def normalise_area(df):\n",
        "  if df[\"area_size\"] < 4323.0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        " \n",
        "def normalise_house():\n",
        "  csv_url = 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "  df_london_housing = pd.read_csv(csv_url)\n",
        "\n",
        "  # To keep things simple, removed nan and empty rows\n",
        "  df_london_housing.dropna(inplace=True, subset=[\"area_size\"])\n",
        "      \n",
        "  area_median = df_london_housing[\"area_size\"].median()\n",
        "  print(f'median = {area_median}')\n",
        "\n",
        "  # Normalise number of houses\n",
        "  df_london_housing[\"area_size_normalised\"] = df_london_housing.apply(normalise_area, axis=1)\n",
        "  print(df_london_housing[\"area_size\"].head(10))\n",
        "  print(df_london_housing[\"area_size_normalised\"].head(10))\n",
        "  #print(df_london_housing[\"area_size_normalised\"].describe())\n",
        "\n",
        "normalise_house()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "median = 4323.0\n",
            "102      315.0\n",
            "103     3780.0\n",
            "104     8675.0\n",
            "105     6429.0\n",
            "106     4323.0\n",
            "107    15013.0\n",
            "108     2179.0\n",
            "109     8650.0\n",
            "110     5554.0\n",
            "111     8220.0\n",
            "Name: area_size, dtype: float64\n",
            "102    0\n",
            "103    0\n",
            "104    1\n",
            "105    1\n",
            "106    1\n",
            "107    1\n",
            "108    0\n",
            "109    1\n",
            "110    1\n",
            "111    1\n",
            "Name: area_size_normalised, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}